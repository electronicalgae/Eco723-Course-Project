from tensorflow import keras

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import os
from datetime import datetime



from pandas_datareader.data import DataReader
import yfinance as yf
from pandas_datareader import data as pdr
data = yf.download("MSFT", start ="2004-01-01", end= "2025-06-30", interval="1d")
data.columns = ['_'.join(col).strip() for col in data.columns.values]

data['log_return']= np.log(data['Close_MSFT']/data['Close_MSFT'].shift(1))
data['seven_day_moving_average'] = data['Close_MSFT'].rolling(window=7).mean()
print(data.head())
print(data.info())
print(data.describe())

plt.figure(figsize=(12,6))
plt.plot(data.index, data['Open_MSFT'], label="Open", color="blue")
plt.xlabel("Date")
plt.ylabel("Price")
plt.title("Opening Price over Time")
plt.legend()
plt.grid(True)
plt.savefig('opening_price_over_time.png')
plt.show()



plt.figure(figsize=(12,6))
plt.plot(data.index, data['Close_MSFT'], label="Close", color="red")
plt.xlabel("Date")
plt.ylabel("Price")
plt.title("Closing Price over Time")
plt.legend()
plt.grid(True)
plt.savefig('closing_price_over_time.png')
plt.show()
plt.figure(figsize=(12,6))
plt.plot(data.index, data['Volume_MSFT'], label="Volume", color="green")
plt.xlabel("Date")
plt.ylabel("Volume")
plt.title("Trading Volume over Time")
plt.legend()
plt.grid(True)
plt.savefig('trading_volume_over_time.png')
plt.show()

stock_close = data.Close_MSFT
dataset = stock_close.values
training_data_len = int(np.ceil(len(dataset) * 0.95))

print(training_data_len)

scaler = StandardScaler()
scaled_data = scaler.fit_transform(dataset.reshape(-1, 1))

training_data = scaled_data[:training_data_len]

x_train, y_train = [],[]

for i in range(60, len(training_data)):
    x_train.append(training_data[i-60:i, 0])
    y_train.append(training_data[i, 0])

x_train, ytrain = np.array(x_train), np.array(y_train)

x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))


model_lstm = keras.models.Sequential()

#layers
#1st layer - LSTM 1
model_lstm.add(keras.layers.LSTM(units=128, return_sequences=True, input_shape=(x_train.shape[1], 1)))

#drop 0.3
model_lstm.add(keras.layers.Dropout(0.1))

#2nd layer - LSTM 2
model_lstm.add(keras.layers.LSTM(units=128, return_sequences=False))

#drop 0.3
model_lstm.add(keras.layers.Dropout(0.1))

#3rd layer - Dense
model_lstm.add(keras.layers.Dense(256, activation="relu"))

#4th layer -Dense
model_lstm.add(keras.layers.Dense(units=1))

model_lstm.summary()

model_lstm.compile(optimizer="adam", loss="mae",metrics=[keras.metrics.RootMeanSquaredError()])

training = model_lstm.fit(x_train, ytrain, epochs=20, batch_size= 32)

#preparing the data
test_data= scaled_data[training_data_len - 60:]

x_test = []
y_test = dataset[training_data_len:]

for i in range(60, len(test_data)):
    x_test.append(test_data[i-60:i, 0])

x_test = np.array(x_test)
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))

predictions = model_lstm.predict(x_test)
predictions = scaler.inverse_transform(predictions)


train = data[:training_data_len]
test = data[training_data_len:]

test = test.copy()
test['Predictions'] = predictions

plt.figure(figsize=(12,6))
plt.plot(train.index, train['Close_MSFT'], label="Train (Actual)", color="blue")
plt.plot(test.index, test['Close_MSFT'], label="Test (Actual)", color="green")
plt.plot(test.index, test['Predictions'], label="Predictions", color="red")
plt.xlabel("Date")
plt.ylabel("Price")
plt.title("Predicted Stock Price")
plt.legend()
plt.grid(True)
plt.savefig('predicted_stock_price_lstm.png')
plt.show()


plt.figure(figsize=(12,6))
plt.plot(test.index, test['Close_MSFT'], label="Test (Actual)", color="blue")
plt.plot(test.index, test['Predictions'], label="Predictions", color="red")
plt.xlabel("Date")
plt.ylabel("Price")
plt.title("Predicted vs Actual Stock Price (Test Data)")
plt.legend()
plt.grid(True)
plt.savefig('predicted_vs_actual_stock_price_lstm.png')
plt.show()




def mean_absolute_percentage_error(y_true, y_pred):
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

mape = mean_absolute_percentage_error(test['Close_MSFT'], test['Predictions'])
print(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")

from sklearn.metrics import mean_squared_error
from math import sqrt

# Calculate RMSE for LSTM
rmse_lstm = sqrt(mean_squared_error(test['Close_MSFT'], test['Predictions']))
print(f"LSTM Root Mean Squared Error (RMSE): {rmse_lstm:.2f}")


Random Forest Regressor model.

from sklearn.ensemble import RandomForestRegressor

# Prepare data for Random Forest
# We will use the same training and test sets as for the LSTM model,
# but the input shape for Random Forest is 2D.
# We can use the 'x_train' and 'x_test' data but reshape them.

x_train_rf = np.reshape(x_train, (x_train.shape[0], x_train.shape[1]))
x_test_rf = np.reshape(x_test, (x_test.shape[0], x_test.shape[1]))

# Create and train the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(x_train_rf, ytrain)

# Make predictions with the Random Forest model
rf_predictions_scaled = rf_model.predict(x_test_rf)

# Inverse transform the predictions to the original scale
rf_predictions = scaler.inverse_transform(rf_predictions_scaled.reshape(-1, 1)).flatten()

# Add the Random Forest predictions to a new dataframe for plotting and evaluation
test_rf = test.copy()
test_rf['RF_Predictions'] = rf_predictions

# Plotting the Random Forest predictions
plt.figure(figsize=(12,6))
plt.plot(train.index, train['Close_MSFT'], label="Train (Actual)", color="blue")
plt.plot(test_rf.index, test_rf['Close_MSFT'], label="Test (Actual)", color="green")
plt.plot(test_rf.index, test_rf['RF_Predictions'], label="Random Forest Predictions", color="orange")
plt.xlabel("Date")
plt.ylabel("Price")
plt.title("Random Forest Predicted Stock Price")
plt.legend()
plt.grid(True)
plt.savefig('random_forest_predicted_stock_price.png')
plt.show()

plt.figure(figsize=(12,6))
plt.plot(test_rf.index, test_rf['Close_MSFT'], label="Test (Actual)", color="green")
plt.plot(test_rf.index, test_rf['RF_Predictions'], label="Random Forest Predictions", color="orange")
plt.xlabel("Date")
plt.ylabel("Price")
plt.title("Random Forest Predicted vs Actual Stock Price (Test Data)")
plt.legend()
plt.grid(True)
plt.savefig('random_forest_predicted_vs_actual_stock_price.png')
plt.show()

# Evaluate the Random Forest model
rf_mape = mean_absolute_percentage_error(test_rf['Close_MSFT'], test_rf['RF_Predictions'])
print(f"Random Forest Mean Absolute Percentage Error (MAPE): {rf_mape:.2f}%")

rf_rmse = sqrt(mean_squared_error(test_rf['Close_MSFT'], test_rf['RF_Predictions']))
print(f"Random Forest Root Mean Squared Error (RMSE): {rf_rmse:.2f}")

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.models import load_model
from sklearn.metrics import mean_absolute_percentage_error

best_rmse = float("inf")
best_model = None
num_iterations = 10
rmse_list = []
def RMSE(y_true, y_pred):
    return np.sqrt(np.mean((y_true - y_pred)**2))
# Function to create a fresh LSTM model
def create_model():
    model = Sequential()
    model.add(LSTM(50, return_sequences=True, input_shape=(60, 1)))
    model_lstm.add(keras.layers.Dropout(0.01))
    model.add(LSTM(50, return_sequences=False))
    model_lstm.add(keras.layers.Dropout(0.01))
    model.add(Dense(25))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# Loop through model training
for p in range(num_iterations):
    model_lstm = create_model()  # Reinitialize model each time

    training = model_lstm.fit(x_train, ytrain, epochs=20, batch_size=32, verbose=0)

    test_data = scaled_data[training_data_len - 60:]
    x_test = []
    y_test = dataset[training_data_len:]

    for i in range(60, len(test_data)):
        x_test.append(test_data[i-60:i, 0])

    x_test = np.array(x_test)
    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))

    predictions = model_lstm.predict(x_test)
    predictions = scaler.inverse_transform(predictions)

    train = data[:training_data_len]
    test = data[training_data_len:].copy()
    test['Predictions'] = predictions
    rmse = RMSE(test['Close_MSFT'], test['Predictions'])
    rmse_list.append(rmse)

    if rmse < best_rmse:
        best_rmse = rmse
        best_model = model_lstm
        best_model.save('best_model.keras')  # Save best model
        print("New Best Model")
        print(f"New Best RMSE: {rmse:.6f}")

best_model = load_model("best_model.keras")

predictions = best_model.predict(x_test)
predictions = scaler.inverse_transform(predictions)
train = data[:training_data_len]
test = data[training_data_len:]

test = test.copy()
test['Predictions'] = predictions
mape = mean_absolute_percentage_error(test['Close_MSFT'], test['Predictions'])
rm = RMSE(test['Close_MSFT'], test['Predictions'])
print(f"best MAPE: {mape:.8f}%")
print(f"best rmse{rm:.6f}")




plt.figure(figsize=(12,6))
plt.plot(test.index, test['Close_MSFT'], label="Test (Actual)", color="blue")
plt.plot(test.index, test['Predictions'], label="Predictions", color="red")
plt.xlabel("Date")
plt.ylabel("Price")
plt.title("Predicted vs Actual Stock Price (Test Data)")
plt.legend()
plt.grid(True)
plt.savefig('predicted_vs_actual_stock_price_best_model.png')
plt.show()

from sklearn.metrics import r2_score

r2 = r2_score(test['Close_MSFT'], test['Predictions'])
print(f"R² Score: {r2:.4f}")


# Residuals = Actual - Predicted
residuals = test['Close_MSFT'] - test['Predictions']
import matplotlib.pyplot as plt

plt.figure(figsize=(14,6))
plt.plot(residuals, color='orange')
plt.title('Residuals Over Time')
plt.xlabel('Time')
plt.ylabel('Error (Actual - Predicted)')
plt.axhline(y=0, color='black', linestyle='--')
plt.show()


mean_rmse = np.mean(rmse_list)
print(f"Mean RMSE over {num_iterations} iterations: {mean_rmse:.6f}")
rmse_list

# prompt: genrate linear regression model for this

import matplotlib.pyplot as plt
# Linear Regression Model
from sklearn.linear_model import LinearRegression

# Prepare data for Linear Regression
# We will use the same x_train_rf and x_test_rf which are already reshaped to 2D
# and ytrain as the target variable.

# Create and train the Linear Regression model
lr_model = LinearRegression()
lr_model.fit(x_train_rf, ytrain)

# Make predictions with the Linear Regression model
lr_predictions_scaled = lr_model.predict(x_test_rf)

# Inverse transform the predictions to the original scale
lr_predictions = scaler.inverse_transform(lr_predictions_scaled.reshape(-1, 1)).flatten()

# Add the Linear Regression predictions to a new dataframe for plotting and evaluation
test_lr = test.copy()
test_lr['LR_Predictions'] = lr_predictions

# Plotting the Linear Regression predictions
plt.figure(figsize=(12,6))
plt.plot(train.index, train['Close_MSFT'], label="Train (Actual)", color="blue")
plt.plot(test_lr.index, test_lr['Close_MSFT'], label="Test (Actual)", color="green")
plt.plot(test_lr.index, test_lr['LR_Predictions'], label="Linear Regression Predictions", color="purple")
plt.xlabel("Date")
plt.ylabel("Price")
plt.title("Linear Regression Predicted Stock Price")
plt.legend()
plt.grid(True)
plt.savefig('linear_regression_predicted_stock_price.png')
plt.show()

plt.figure(figsize=(12,6))
plt.plot(test_lr.index, test_lr['Close_MSFT'], label="Test (Actual)", color="green")
plt.plot(test_lr.index, test_lr['LR_Predictions'], label="Linear Regression Predictions", color="purple")
plt.xlabel("Date")
plt.ylabel("Price")
plt.title("Linear Regression Predicted vs Actual Stock Price (Test Data)")
plt.legend()
plt.grid(True)
plt.savefig('linear_regression_predicted_vs_actual_stock_price.png')
plt.show()

# Evaluate the Linear Regression model
lr_mape = mean_absolute_percentage_error(test_lr['Close_MSFT'], test_lr['LR_Predictions'])
print(f"Linear Regression Mean Absolute Percentage Error (MAPE): {lr_mape:.2f}%")

lr_rmse = sqrt(mean_squared_error(test_lr['Close_MSFT'], test_lr['LR_Predictions']))
print(f"Linear Regression Root Mean Squared Error (RMSE): {lr_rmse:.2f}")

lr_r2 = r2_score(test_lr['Close_MSFT'], test_lr['LR_Predictions'])
print(f"Linear Regression R² Score: {lr_r2:.4f}")

# Residuals for Linear Regression
residuals_lr = test_lr['Close_MSFT'] - test_lr['LR_Predictions']

plt.figure(figsize=(14,6))
plt.plot(residuals_lr, color='purple')
plt.title('Linear Regression Residuals Over Time')
plt.xlabel('Time')
plt.ylabel('Error (Actual - Predicted)')
plt.axhline(y=0, color='black', linestyle='--')
plt.savefig('linear_regression_residuals_over_time.png')
plt.show()


Model evalution

print("for LSTM model")
print(f"best MAPE: {mape:.8f}%")
print(f"best rmse{rm:.8f}")
print(f"best R2: {r2:.8f}")
print("for Random Forest model")
print(f"Random Forest Mean Absolute Percentage Error (MAPE): {rf_mape:.8f}%")
print(f"Random Forest Root Mean Squared Error (RMSE): {rf_rmse:.8f}")
print("for Linear Regression model")
print(f"Linear Regression Mean Absolute Percentage Error (MAPE): {lr_mape:.8f}%")
print(f"Linear Regression Root Mean Squared Error (RMSE): {lr_rmse:.8f}")

Based on the results it is evident that linear regression based model outperforms other models, while LSTM model being an alternative choice for this problem however LSTM model is computationally expensive.

